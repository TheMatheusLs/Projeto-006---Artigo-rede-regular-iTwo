{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo na Versão 04 V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gymnasium version 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(f\"Using gymnasium version {gym.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stable-baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"Using stable-baselines3 version {stable_baselines3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment.Settings import *\n",
    "from Enviroment.Manager import Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version 2.2.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "print(f\"Using torch version {th.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.pb_history = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        # Acessa o ambiente\n",
    "        env = self.training_env.envs[0].env\n",
    "\n",
    "        info = env.get_custom_info()\n",
    "\n",
    "        if info[\"total_requests\"] > 39998:\n",
    "            self.logger.record(\"custom_on_episode/total_requests\", info[\"total_requests\"])\n",
    "            self.logger.record(\"custom_on_episode/blocking_probability\", info[\"blocking_probability\"])\n",
    "            self.pb_history.append(info[\"blocking_probability\"])\n",
    "            self.logger.record(\"custom_on_episode/blocking_probability_total_mean\", np.mean(self.pb_history))\n",
    "\n",
    "            # Salva a média da probabilidade de bloqueio dos últimos 10 episódios\n",
    "            self.logger.record(\"custom_on_episode/blocking_probability_mean_10\", np.mean(self.pb_history[-10:]))\n",
    "\n",
    "\n",
    "            self.logger.record(\"custom_on_episode/reward_episode\", info[\"reward_episode\"])\n",
    "            self.logger.record(\"custom_on_episode/total_number_of_blocks\", info[\"total_number_of_blocks\"])\n",
    "            self.logger.record(\"custom_on_episode/calls_SAR\", env.SAR_calls)\n",
    "\n",
    "        # Verifica se o episódio terminou\t\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the enviroment {'Observation': 'availability-vector', 'Action': 'RSA-SAR', 'Reward': 'RL-defaut', 'StopCond': '40kReqs', 'StartCond': 'Empty'}\n",
      "Using the log path D:\\98_phD_Files\\Simulator-RSA-SAR-RL-GA\\logs\\PPO\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">d:\\98_phD_Files\\Simulator-RSA-SAR-RL-GA\\venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "d:\\98_phD_Files\\Simulator-RSA-SAR-RL-GA\\venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configura como deve ser o ambiente\n",
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=270,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"Train_PPO_40kReqs_300_v11\",\n",
    ")\n",
    "\n",
    "# Cria o log path\n",
    "LOG_PATH = \"D:\\\\98_phD_Files\\\\Simulator-RSA-SAR-RL-GA\\\\logs\\\\PPO\\\\\"\n",
    "\n",
    "print(f\"Using the enviroment {enviroment_type}\")\n",
    "\n",
    "print(f\"Using the log path {LOG_PATH}\")\n",
    "\n",
    "# Criando o diretório para o monitoramento\n",
    "monitor_dir = LOG_PATH + \"\\\\monitor_logs\"\n",
    "os.makedirs(monitor_dir, exist_ok=True)\n",
    "\n",
    "# Criando o monitor para monitorar o ambiente\n",
    "env = Monitor(env, monitor_dir)\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(\n",
    "    activation_fn=th.nn.LeakyReLU,\n",
    "    net_arch=dict(pi=[512, 128], vf=[512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0, \n",
    "    tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\', \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Callback de avaliação\n",
    "eval_callback = EvalCallback(\n",
    "    env, \n",
    "    eval_freq=40_000, \n",
    "    deterministic=True, \n",
    "    best_model_save_path=LOG_PATH + '\\\\training\\\\best_model',\n",
    "    log_path=LOG_PATH + '\\\\training\\\\logs',\n",
    "    render=False, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=40_000_000, \n",
    "    callback=[TensorboardCallback(), eval_callback],\n",
    "    progress_bar=True, \n",
    "    tb_log_name=\"Model_v11\"\n",
    ")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
