{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando modelos com o agente em PPO\n",
    "\n",
    "Documentação do [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) para referência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os pacotes necessários e construíndo os callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gymnasium version 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(f\"Using gymnasium version {gym.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stable-baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "print(f\"Using stable-baselines3 version {stable_baselines3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o callback para salvar o melhor modelo durante o treinamento \n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback que salva o melhor modelo baseado na recompensa média de treinamento.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Cria o diretório de log se não existir\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Avalia o modelo\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Calcula a média da recompensa de treinamento\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                # Se a média da recompensa for maior que a melhor média de recompensa\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment.Settings import *\n",
    "from Enviroment.Manager import Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambientes para os resultados de baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(env, model, num_sim=10):\n",
    "    ## Retorna a PB para o modelo treinado\n",
    "    print(\"Testing the model...\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "    seeds = np.random.randint(0, 100_000, num_sim, dtype=int)\n",
    "    pbs = np.zeros(num_sim)\n",
    "    reward = np.zeros(num_sim)\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "\n",
    "        print(f\"Executando simulação {i+1} de {num_sim}\")\n",
    "\n",
    "        # Reseta o ambiente\n",
    "        state, info = env.reset(int(seed))\n",
    "\n",
    "        for reqs in range(MAX_REQS+1):\n",
    "\n",
    "            if int == type(model):\n",
    "                alg_heuristic = model\n",
    "            else:\n",
    "                alg_heuristic = model.predict(observation=state, deterministic=True)[0]\n",
    "\n",
    "            state, _, done, trunk, info = env.step(alg_heuristic)\n",
    "\n",
    "            if (done or trunk) and reward[i] == 0:\n",
    "                reward[i] = env._reward_episode\n",
    "\n",
    "        pbs[i] = info['total_number_of_blocks'] / reqs\n",
    "\n",
    "        print(f\"Blocking Probability: {pbs[i]} | Reward: {reward[i]} | Req: {reqs}\")\n",
    "\n",
    "    print(f\"\\nBlocking Probability: {np.mean(pbs)} | Min: {np.min(pbs)} | Max: {np.max(pbs)} | +- {np.std(pbs)}\")\n",
    "    print(f\"Reward: {np.mean(reward)} | Min: {np.min(reward)} | Max: {np.max(reward)} | +- {np.std(reward)}\")\n",
    "\n",
    "    return pbs, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados para RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.00268 | Reward: 99464.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.00311 | Reward: 99378.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.00302 | Reward: 99396.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.00275 | Reward: 99450.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.00311 | Reward: 99378.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.00348 | Reward: 99304.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.00304 | Reward: 99392.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.00336 | Reward: 99328.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.00271 | Reward: 99458.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.00349 | Reward: 99302.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.0030750000000000005 | Min: 0.00268 | Max: 0.00349 | +- 0.0002861904959987316\n",
      "Reward: 99385.0 | Min: 99302.0 | Max: 99464.0 | +- 57.238099199746316\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Avaliando a PB do modelo treinado\n",
    "enviroment_type_test = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"MaxReq\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"RSA_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, 0, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados para SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.00089 | Reward: 99822.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.00119 | Reward: 99762.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.00142 | Reward: 99716.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.00095 | Reward: 99810.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.00124 | Reward: 99752.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.0013 | Reward: 99740.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.00144 | Reward: 99712.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.00164 | Reward: 99672.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.00081 | Reward: 99838.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.00151 | Reward: 99698.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.001239 | Min: 0.00081 | Max: 0.00164 | +- 0.00026512072721686624\n",
      "Reward: 99752.2 | Min: 99672.0 | Max: 99838.0 | +- 53.02414544337325\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"SAR_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, 1, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o primeiro ambiente para os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cf1cc4933b48e8a41edcdd03922f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o modelo\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\')\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=40_000, log_dir=LOG_PATH + \"\\\\training\\\\\", verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=1_800_000, callback=callback, progress_bar=True)\n",
    "#model.learn(total_timesteps=1_800_000, callback=callback, progress_bar=True, tb_log_name=\"first_run\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 39894.80 +/- 28.29\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../logs/PPO_003'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.00156 | Reward: 99688.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.00135 | Reward: 99730.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.00137 | Reward: 99726.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.00111 | Reward: 99778.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.00164 | Reward: 99672.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.00135 | Reward: 99730.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.00133 | Reward: 99734.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.0017 | Reward: 99660.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.00118 | Reward: 99764.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.00202 | Reward: 99596.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.001461 | Min: 0.00111 | Max: 0.00202 | +- 0.00025762181584640685\n",
      "Reward: 99707.8 | Min: 99596.0 | Max: 99778.0 | +- 51.524363169281386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00156, 0.00135, 0.00137, 0.00111, 0.00164, 0.00135, 0.00133,\n",
       "        0.0017 , 0.00118, 0.00202]),\n",
       " array([99688., 99730., 99726., 99778., 99672., 99730., 99734., 99660.,\n",
       "        99764., 99596.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"PPO_Eval\",\n",
    ")\n",
    "\n",
    "model = PPO.load(LOG_PATH + '\\\\training\\\\best_model.zip')\n",
    "\n",
    "## Retorna a PB para o modelo treinado\n",
    "run_test(env, model, num_sim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o segundo ambiente para os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02207896fa024c25b0989db2485db811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"Route\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[1024, 512, 128], vf=[1024, 512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\')\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=1_800_000, callback=callback, progress_bar=True, tb_log_name=\"v1_Route\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 38755.40 +/- 54.02\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o terceiro ambiente para os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2de819743646e397508bace24846e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[1024, 512, 128], vf=[1024, 512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\')\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=4_800_000, callback=callback, progress_bar=True, tb_log_name=\"v2_Route\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 39832.20 +/- 42.83\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../logs/PPO_004'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=415, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.00068 | Reward: 99864.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.00104 | Reward: 99792.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.00088 | Reward: 99824.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.00085 | Reward: 99830.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.00119 | Reward: 99762.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.00113 | Reward: 99774.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.00099 | Reward: 99802.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.0013 | Reward: 99740.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.00081 | Reward: 99838.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.0014 | Reward: 99720.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.001027 | Min: 0.00068 | Max: 0.0014 | +- 0.00021744194627532196\n",
      "Reward: 99794.6 | Min: 99720.0 | Max: 99864.0 | +- 43.48838925506439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00068, 0.00104, 0.00088, 0.00085, 0.00119, 0.00113, 0.00099,\n",
       "        0.0013 , 0.00081, 0.0014 ]),\n",
       " array([99864., 99792., 99824., 99830., 99762., 99774., 99802., 99740.,\n",
       "        99838., 99720.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"PPO_Eval\",\n",
    ")\n",
    "\n",
    "model = PPO.load(LOG_PATH + '\\\\training\\\\best_model\\\\best_model.zip')\n",
    "\n",
    "## Retorna a PB para o modelo treinado\n",
    "run_test(env, model, num_sim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = '..\\logs\\PPO_002_good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "model = PPO.load(LOG_PATH + '\\\\training\\\\final_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.env = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e03cb724494135bd85e9c06cd8700c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tried to step environment that needs reset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Treina o modelo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4_800_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcont_RSA-SAR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Salva o modelo treinado\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(LOG_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:93\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mStep the environment with the given action\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m:param action: the action\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m:return: observation, reward, terminated, truncated, information\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tried to step environment that needs reset"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<Monitor<Enviroment instance>> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000018F1F625790>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    }
   ],
   "source": [
    "# Treina o modelo\n",
    "model.learn(total_timesteps=4_800_000, callback=callback, progress_bar=True, tb_log_name=\"cont_RSA-SAR\", reset_num_timesteps=False)\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de informações Extra no Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        env = self.model.get_env()\n",
    "        \n",
    "        # Plotando a probabilidade de bloqueio\n",
    "        self.logger.record(\"blocking_probability\", env._total_number_of_blocks / env._last_request)\n",
    "\n",
    "\n",
    "        value = np.random.random()\n",
    "        self.logger.record(\"random_value\", value)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8101bd2e645019bb3b4efbc683055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute '_total_number_of_blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m callback22 \u001b[38;5;241m=\u001b[39m TensorboardCallback()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Treina o modelo\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback22\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv3_Route\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Salva o modelo treinado\u001b[39;00m\n\u001b[0;32m     39\u001b[0m model2\u001b[38;5;241m.\u001b[39msave(LOG_PATH2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:200\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    199\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:219\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    216\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:219\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    216\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[1;32md:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mTensorboardCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_env()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Plotando a probabilidade de bloqueio\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocking_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_total_number_of_blocks\u001b[49m \u001b[38;5;241m/\u001b[39m env\u001b[38;5;241m.\u001b[39m_last_request)\n\u001b[0;32m     17\u001b[0m value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, value)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute '_total_number_of_blocks'"
     ]
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env2 = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH2 = env2.folder_name\n",
    "\n",
    "env2 = Monitor(env2, LOG_PATH2 + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[512, 512, 128], vf=[512, 512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model2 = PPO(\"MlpPolicy\", env2, policy_kwargs=policy_kwargs, verbose=0, tensorboard_log=LOG_PATH2 + '\\\\tensorboard\\\\')\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback2 = EvalCallback(env2, best_model_save_path=LOG_PATH2 + '\\\\training\\\\best_model', log_path=LOG_PATH2 + '\\\\training\\\\logs', eval_freq=20_000, deterministic=True, render=False, verbose=0)\n",
    "callback22 = TensorboardCallback()\n",
    "\n",
    "\n",
    "# Treina o modelo\n",
    "model2.learn(total_timesteps=100_000, callback=[callback2, callback22], progress_bar=True, tb_log_name=\"v3_Route\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model2.save(LOG_PATH2 + '\\\\training\\\\final_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhDVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
