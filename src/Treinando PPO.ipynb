{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch as th\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gymnasium version 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(f\"Using gymnasium version {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stable-baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "print(f\"Using stable-baselines3 version {stable_baselines3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment.Settings import *\n",
    "from Enviroment.Manager import Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(env, model, num_sim=10):\n",
    "    ## Retorna a PB para o modelo treinado\n",
    "    print(\"Testing the model...\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "    seeds = np.random.randint(0, 100_000, num_sim, dtype=int)\n",
    "    pbs = np.zeros(num_sim)\n",
    "    reward = np.zeros(num_sim)\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "\n",
    "        print(f\"Executando simulação {i+1} de {num_sim}\")\n",
    "\n",
    "        # Reseta o ambiente\n",
    "        state, info = env.reset(int(seed))\n",
    "\n",
    "        for reqs in range(MAX_REQS+1):\n",
    "\n",
    "            if int == type(model):\n",
    "                alg_heuristic = model\n",
    "            else:\n",
    "                alg_heuristic = model.predict(observation=state, deterministic=False)[0]\n",
    "\n",
    "            state, _, done, trunk, info = env.step(alg_heuristic)\n",
    "\n",
    "            if (done or trunk) and reward[i] == 0:\n",
    "                reward[i] = env._reward_episode\n",
    "\n",
    "        pbs[i] = info['total_number_of_blocks'] / reqs\n",
    "\n",
    "        print(f\"Blocking Probability: {pbs[i]} | Reward: {reward[i]} | Req: {reqs}\")\n",
    "\n",
    "    print(f\"\\nBlocking Probability: {np.mean(pbs)} | Min: {np.min(pbs)} | Max: {np.max(pbs)} | +- {np.std(pbs)}\")\n",
    "    print(f\"Reward: {np.mean(reward)} | Min: {np.min(reward)} | Max: {np.max(reward)} | +- {np.std(reward)}\")\n",
    "\n",
    "    return pbs, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a PB do modelo treinado\n",
    "enviroment_type_test = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"MaxReq\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.01135 | Reward: 97730.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.01162 | Reward: 97676.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.01227 | Reward: 97546.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.01056 | Reward: 97888.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.01148 | Reward: 97704.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.01365 | Reward: 97270.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.01252 | Reward: 97496.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.01258 | Reward: 97484.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.01118 | Reward: 97764.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.01259 | Reward: 97482.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.011980000000000001 | Min: 0.01056 | Max: 0.01365 | +- 0.0008561308311233745\n",
      "Reward: 97604.0 | Min: 97270.0 | Max: 97888.0 | +- 171.2261662246749\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=300,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"SAR_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, 1, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float, final_value) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule from `initial_value` to `final_value` over `progress_remaining` fraction of steps.\n",
    "\n",
    "    :param initial_value: initial learning rate\n",
    "    :param final_value: final learning rate\n",
    "    :return: schedule function\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        return initial_value + (final_value - initial_value) * progress_remaining\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs will be saved at ../logs/PPO_001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bd0dd3da9e4a3b854fa3ccf16f72d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "print(f\"Logs will be saved at {LOG_PATH}\")\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[1024, 512, 128], vf=[1024, 512, 128]))\n",
    "\n",
    "# Cria o modelo de treinamento PPO com decaimento do learning rate\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=0, \n",
    "            tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\',\n",
    "            learning_rate=linear_schedule(0.0008, 0.00005))\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=12_800_000, callback=callback, progress_bar=True, tb_log_name=\"V0_RSA-SAR\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('../logs/PPO_001' + '\\\\training\\\\best_model\\\\best_model.zip', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs will be saved at ../logs/PPO_003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33075a1a2654c7389f1e5855aefb6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "print(f\"Logs will be saved at {LOG_PATH}\")\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[1024, 512, 128], vf=[1024, 512, 128]))\n",
    "\n",
    "# Cria o modelo de treinamento PPO com decaimento do learning rate\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=0, \n",
    "            tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\',\n",
    "            learning_rate=linear_schedule(0.0003, 0.0002))\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=5_000_000, callback=callback, progress_bar=True, tb_log_name=\"V1_RSA-SAR\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PPO.load(LOG_PATH + '\\\\training\\\\best_model\\\\best_model.zip', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.01135 | Reward: 97730.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.01162 | Reward: 97676.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.01227 | Reward: 97546.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.01056 | Reward: 97888.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.01148 | Reward: 97704.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.01365 | Reward: 97270.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.01252 | Reward: 97496.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.01258 | Reward: 97484.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.01118 | Reward: 97764.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.01259 | Reward: 97482.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.011980000000000001 | Min: 0.01056 | Max: 0.01365 | +- 0.0008561308311233745\n",
      "Reward: 97604.0 | Min: 97270.0 | Max: 97888.0 | +- 171.2261662246749\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=300,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"Model_003_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, best_model, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.01135 | Reward: 97730.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.01162 | Reward: 97676.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.01227 | Reward: 97546.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.01056 | Reward: 97888.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.01148 | Reward: 97704.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.01365 | Reward: 97270.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.01252 | Reward: 97496.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.01258 | Reward: 97484.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.01118 | Reward: 97764.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.01259 | Reward: 97482.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.011980000000000001 | Min: 0.01056 | Max: 0.01365 | +- 0.0008561308311233745\n",
      "Reward: 97604.0 | Min: 97270.0 | Max: 97888.0 | +- 171.2261662246749\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=300,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"Model_003_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, best_model, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"ODD-one-hot\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0609f30a9c401a9d376b3461f8d26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs will be saved at ../logs/PPO_004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "print(f\"Logs will be saved at {LOG_PATH}\")\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512, 128], vf=[512, 128]))\n",
    "\n",
    "# Cria o modelo de treinamento PPO com decaimento do learning rate\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=0, \n",
    "            tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\',\n",
    "            learning_rate=0.0001)\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=2_000_000, callback=callback, progress_bar=True, tb_log_name=\"V4_RSA-SAR_ODD\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238f7190ef3b4f2aa2f9120bae8a63b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs will be saved at ../logs/PPO_teste_005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO_teste\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "print(f\"Logs will be saved at {LOG_PATH}\")\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=[512, 128], vf=[512, 128]))\n",
    "\n",
    "\n",
    "# Cria o modelo de treinamento PPO com decaimento do learning rate\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=0, \n",
    "            tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\',\n",
    "            learning_rate=0.0002,\n",
    "            n_steps=256,\n",
    "            batch_size=512,\n",
    "            n_epochs=10,\n",
    "            gae_lambda=0.95,\n",
    "            gamma=0.99,\n",
    "            clip_range=0.2,\n",
    "            ent_coef= 0.01,\n",
    "            vf_coef= 0.5,\n",
    "            max_grad_norm=0.5)\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0, n_eval_episodes=3)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=100_000, callback=callback, progress_bar=True, tb_log_name=\"V5_RSA-SAR_ODD\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84a7250fba24fbbb6aeb30e21fb168c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Treina o modelo\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, progress_bar=True, tb_log_name=\"V5_RSA-SAR_ODD\", reset_num_timesteps=False)\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PPO.load(LOG_PATH + '\\\\training\\\\best_model\\\\best_model.zip', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Executando simulação 1 de 10\n",
      "Blocking Probability: 0.01158 | Reward: 97684.0 | Req: 100000\n",
      "Executando simulação 2 de 10\n",
      "Blocking Probability: 0.01313 | Reward: 97374.0 | Req: 100000\n",
      "Executando simulação 3 de 10\n",
      "Blocking Probability: 0.01256 | Reward: 97488.0 | Req: 100000\n",
      "Executando simulação 4 de 10\n",
      "Blocking Probability: 0.01163 | Reward: 97674.0 | Req: 100000\n",
      "Executando simulação 5 de 10\n",
      "Blocking Probability: 0.01244 | Reward: 97512.0 | Req: 100000\n",
      "Executando simulação 6 de 10\n",
      "Blocking Probability: 0.01362 | Reward: 97276.0 | Req: 100000\n",
      "Executando simulação 7 de 10\n",
      "Blocking Probability: 0.01318 | Reward: 97364.0 | Req: 100000\n",
      "Executando simulação 8 de 10\n",
      "Blocking Probability: 0.01341 | Reward: 97318.0 | Req: 100000\n",
      "Executando simulação 9 de 10\n",
      "Blocking Probability: 0.01219 | Reward: 97562.0 | Req: 100000\n",
      "Executando simulação 10 de 10\n",
      "Blocking Probability: 0.01349 | Reward: 97302.0 | Req: 100000\n",
      "\n",
      "Blocking Probability: 0.012723000000000002 | Min: 0.01158 | Max: 0.01362 | +- 0.000716324647070028\n",
      "Reward: 97455.4 | Min: 97276.0 | Max: 97684.0 | +- 143.2649294140056\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Avaliando a PB do modelo treinado\n",
    "enviroment_type_test = {\n",
    "    \"Observation\": \"ODD-one-hot\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"MaxReq\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=300,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type_test,\n",
    "    data_folder=\"Model_003_eval\",\n",
    ")\n",
    "\n",
    "run_test(env, best_model, num_sim=10) # Executa o RSA 10 vezes para calcular a PB\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO_param\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "print(f\"Logs will be saved at {LOG_PATH}\")\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=[512, 128], vf=[512, 128]))\n",
    "\n",
    "\n",
    "# Cria o modelo de treinamento PPO com decaimento do learning rate\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=0, \n",
    "            tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\',\n",
    "            learning_rate=0.0004,\n",
    "            n_steps=128,\n",
    "            batch_size=2048,\n",
    "            n_epochs=10,\n",
    "            gae_lambda=0.95,\n",
    "            gamma=0.99,\n",
    "            clip_range=0.05,\n",
    "            ent_coef= 0.05,\n",
    "            vf_coef= 0.5,\n",
    "            max_grad_norm=0.5)\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0, n_eval_episodes=3)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=100_000, callback=callback, progress_bar=True, tb_log_name=\"V5_RSA-SAR_vector\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
