{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gymnasium version 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(f\"Using gymnasium version {gym.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stable-baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "print(f\"Using stable-baselines3 version {stable_baselines3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment.Settings import *\n",
    "from Enviroment.Manager import Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 256`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 256\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=256 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0571b70169bc46fbaf8667f9855c62a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"availability-vector\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=[512, 512, 128], vf=[512, 512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\', seed=42,\n",
    "            learning_rate=0.00025, \n",
    "            n_steps=256, \n",
    "            batch_size=4096, \n",
    "            n_epochs=10, \n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            ent_coef=0.005)\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=4_800_000, callback=callback, progress_bar=True, tb_log_name=\"RL_RSA-SAR_vector_v0\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10be87588d944f38d195dcdaba17a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\98_phD_Files\\Projeto 006 - Artigo rede regular iTwo\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 128`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 128\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=128 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enviroment_type = {\n",
    "    \"Observation\": \"ODD-one-hot\",\n",
    "    \"Action\": \"RSA-SAR\",\n",
    "    \"Reward\": \"RL-defaut\",\n",
    "    \"StopCond\": \"40kReqs\",\n",
    "    \"StartCond\": \"Empty\"\n",
    "}\n",
    "\n",
    "# Cria o ambiente de simulação\n",
    "env = Enviroment(\n",
    "    network_load=LOAD,\n",
    "    k_routes=K_ROUTES,\n",
    "    number_of_slots=NUMBER_OF_SLOTS,\n",
    "    enviroment_type=enviroment_type,\n",
    "    data_folder=\"PPO\",\n",
    ")\n",
    "\n",
    "LOG_PATH = env.folder_name\n",
    "\n",
    "env = Monitor(env, LOG_PATH + '\\\\training\\\\training')\n",
    "\n",
    "# Cria o dicionário com as configurações da política da rede. \n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=[512, 128], vf=[512, 128]))\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, tensorboard_log=LOG_PATH + '\\\\tensorboard\\\\', seed=42,\n",
    "            learning_rate=0.00005, \n",
    "            n_steps=128, \n",
    "            batch_size=4096, \n",
    "            n_epochs=4,\n",
    "            gamma=0.9,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            ent_coef=0.01)\n",
    "\n",
    "\n",
    "# Cria o callback para salvar o melhor modelo\n",
    "callback = EvalCallback(env, best_model_save_path=LOG_PATH + '\\\\training\\\\best_model', log_path=LOG_PATH + '\\\\training\\\\logs', eval_freq=40_000, deterministic=True, render=False, verbose=0)\n",
    "\n",
    "# Treina o modelo\n",
    "model.learn(total_timesteps=5_800_000, callback=callback, progress_bar=True, tb_log_name=\"RL_RSA-SAR_vector_v0\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "model.save(LOG_PATH + '\\\\training\\\\final_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
